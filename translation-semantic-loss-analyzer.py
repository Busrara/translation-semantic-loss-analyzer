# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

from transformers import MarianMTModel, MarianTokenizer, BertTokenizer, BertModel
import torch
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm.notebook import tqdm

class TranslationAnalyzer:
  def __init__(self):
    # Upload Bert model for the semantic representations.
    print("Bert model is uploading...")
    self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
    self.bert_model = BertModel.from_pretrained('bert-base-multilingual-cased')

    # Models for different languages
    self.language_pairs = {
            'English-Spanish': 'Helsinki-NLP/opus-mt-en-es',
            'English-French': 'Helsinki-NLP/opus-mt-en-fr',
            'English-German': 'Helsinki-NLP/opus-mt-en-de',
            'English-Italian': 'Helsinki-NLP/opus-mt-en-it',
            'English-Portugese': 'Helsinki-NLP/opus-mt-en-ROMANCE'  # Bu model birden fazla Roman dili destekler
        }

    self.translation_models = {}
    self.translation_tokenizers = {}

  def load_translation_models(self):
        for lang_pair, model_name in self.language_pairs.items():
            try:
                print(f"For {lang_pair}, {model_name} is uploading")
                self.translation_tokenizers[lang_pair] = MarianTokenizer.from_pretrained(model_name)
                self.translation_models[lang_pair] = MarianMTModel.from_pretrained(model_name)
                print(f"{lang_pair} is uploaded successfully.")
            except Exception as e:
                print(f"Error: {lang_pair} is not uploaded. {str(e)}")

  def translate_text(self, text, lang_pair):
   try:
    tokenizer = self.translation_tokenizers[lang_pair]
    model = self.translation_models[lang_pair]

    inputs = tokenizer([text], return_tensors="pt", padding=True, truncation=True, max_length=512)
    translated = model.generate(**inputs)
    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)
    return translated_text
   except KeyError:
    return f"No model for this language pair."
   except Exception as e:
    return f"Error: {str(e)}"

  def get_bert_embedding(self,text):
    try:
      inputs = self.bert_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
      with torch.no_grad():
        outputs = self.bert_model(**inputs)

      return outputs.last_hidden_state[:, 0, :].squeeze().numpy()
    except Exception as er:
      print(f"Error: {str(e)}")
      return None

  def calculate_cosine_similarity(self,vec1,vec2):
    if vec1 is None or vec2 is None:
      return 0.0

    vec1 = vec1.flatten()
    vec2 = vec2.flatten()

    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)

    if norm1==0 or norm2==0:
      return 0.0
    return dot_product / (norm1*norm2)

  def analyze_translations(self, source_texts):
    results = []

    for i, source_text in enumerate(tqdm(source_texts, desc="Analyzing the texts.")):
     print(f"\nMetin {i+1}: '{source_text}'")
     source_embedding = self.get_bert_embedding(source_text)

     text_results = []

     for lang_pair in self.language_pairs.keys():
       translated_text = self.translate_text(source_text, lang_pair)
       print(f"{lang_pair} translated: '{translated_text}'")
       translated_embedding = self.get_bert_embedding(translated_text)
       similarity = self.calculate_cosine_similarity(source_embedding, translated_embedding)

       text_results.append({
                    'source_text': source_text,
                    'lang_pair': lang_pair,
                    'Translation': translated_text,
                    'similarity_score': similarity

       })
     results.extend(text_results)

    return pd.DataFrame(results)

  def visualize_results(self,result_df,plot_type='bar'):
    avg_scores = result_df.groupby('lang_pair')['similarity_score'].mean().sort_values(ascending=False).reset_index()
    avg_scores = avg_scores.sort_values(by='similarity_score', ascending=False)
    plt.figure(figsize=(12,8))

    if plot_type=='bar':
      ax = sns.barplot(x='similarity_score', y='lang_pair', data=avg_scores,
                 palette='viridis', orient='h')

      for i,v in enumerate(avg_scores['similarity_score']):
        ax.text(v + 0.01, i, f'{v:.3f}', va='center')

      plt.xlim(0,1.05)
      plt.title('Similarity Scores Between Different Language Pairs')
      plt.xlabel('Semantic Similarity Score', fontsize=12)
      plt.ylabel('Lang Pair', fontsize=12)
      plt.grid(True, axis='x', linestyle='--', alpha=0.6)

    elif plot_type =='heatmap':
      pivot_df =result_df.pivot_table(index='source_text',
                                       columns='lang_pair',
                                       values='similarity_score')
      sns.heatmap(pivot_df, annot=True, cmap='YlGnBu', fmt='.3f', linewidths=.5)
      plt.title('Similarity Scores Between Diffrent Language Pairs and Texts', fontsize=16)
      plt.tight_layout()

    return plt

if __name__=="__main__":
  analyzer = TranslationAnalyzer()
  analyzer.load_translation_models()
  test_texts = [
        "The artificial intelligence revolution is transforming our world in unprecedented ways.",
        "Climate change represents one of the greatest challenges facing humanity today.",
        "Cultural understanding is essential for effective international communication.",
        "Access to quality education should be a fundamental right for every child.",
        "The global economy is increasingly interconnected through digital technologies."
    ]

  results = analyzer.analyze_translations(test_texts)
  plot = analyzer.visualize_results(results, plot_type='bar')
  plot.savefig('translation_similarity_bar.png', dpi=300, bbox_inches='tight')

  plot = analyzer.visualize_results(results, plot_type='heatmap')
  plot.savefig('translation_similarity_heatmap.png', dpi=300, bbox_inches='tight')

  results.to_csv('translation_analysis_results.csv', index=False)

print("Analyze has been successfully completed!")